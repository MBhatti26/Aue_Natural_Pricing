# ğŸ›ï¸ Aue Natural Price Monitoring Data Pipeline

**Automated data processing pipeline for collecting and analyzing beauty product pricing data from Google Shopping via Oxylabs.**

---

## ğŸš€ Quick Start

### For New Data Processing (Most Common)
```bash
# Run this command whenever new data has been scraped
python scripts/process_new_data.py
```

### For Product Matching & Price Comparison
```bash
# Run the enhanced matching engine with vector embeddings
python scripts/run_production_matching.py
```

### First Time Setup
```bash
# One-time setup
chmod +x setup.sh
./setup.sh
```

---

## ğŸ“‹ What This Pipeline Does

1. **ğŸ“¥ Data Collection**: Scrapes Google Shopping data using Oxylabs API
2. **ğŸ”„ Data Processing**: Normalizes raw data into database-ready CSV files  
3. **âœ… Data Validation**: Ensures data quality and relationships
4. **ğŸ“Š Database Preparation**: Creates clean CSV files ready for import
5. **ğŸ¤ Product Matching**: Uses enhanced semantic matching with vector embeddings for price comparison
6. **ğŸ“ˆ Business Intelligence**: Generates consolidated outputs for analytics and reporting

---

## ğŸ—‚ï¸ File Organization

### Input Data
- `data/raw/all_search_results_YYYYMMDD_HHMMSS.csv` - Raw Oxylabs results
- Contains ~1,000+ product records across 4 categories

### Processed Data  
- `data/processed/` - **Latest timestamped outputs (one per run)**
  - `processed_matches_YYYYMMDD_HHMMSS.csv` - Main matching results
  - `unmatched_products_YYYYMMDD_HHMMSS.csv` - Unmatched products 
  - `processing_summary_YYYYMMDD_HHMMSS.json` - Processing metadata
  - `embeddings_cache.pkl` - Cached vector embeddings for performance
  - `archive_old_files/` - Historical files and intermediates
- `database_to_import/` - **Final CSV files ready for database import**

### Scripts
- `scripts/process_new_data.py` - **Main automation script**
- `scripts/run_production_matching.py` - **Enhanced product matching pipeline**
- `scripts/post_process_unmatched.py` - Additional match recovery
- `scripts/consolidate_outputs.py` - BI-ready output generation
- `scripts/validate_data.py` - Data quality validation
- `scripts/update_database_files.py` - Manual file update helper

---

## ğŸ“Š Database Schema

### Tables Created
```
ğŸ“ brand_name.csv        â†’ Unique product brands (681 brands)
ğŸ“ category_name.csv     â†’ Product categories (4 categories)  
ğŸ“ price_source.csv      â†’ Data sources (Oxylabs Google Shopping)
ğŸ“ retailer_name.csv     â†’ Online retailers (370+ retailers)
ğŸ“ product.csv           â†’ Products with relationships (939 products)
ğŸ“ price_history.csv     â†’ Price records with timestamps (1,033+ prices)
```

### Key Relationships
```
price_source â”€â”€â†’ retailer_name â”€â”€â†’ price_history
brand_name â”€â”€â†’ product â”€â”€â†’ price_history  
category_name â”€â”€â†’ product
```

---

## ğŸ¯ For Next Time You Scrape New Data

### Simple Process (3 commands):
```bash
# 1. Activate environment  
source venv/bin/activate

# 2. Process new data (finds latest automatically)
python scripts/process_new_data.py

# 3. Validate everything is correct
python scripts/validate_data.py
```

### What These Commands Do:
1. **Finds** the latest raw data file automatically
2. **Processes** it through the cleaning pipeline  
3. **Updates** all CSV files in `database_to_import/`
4. **Validates** data quality and relationships
5. **Generates** a processing report

---

## ğŸ“ˆ Expected Data Volumes

| Category | Records | Description |
|----------|---------|-------------|
| **Shampoo Bar** | ~260 products | Solid shampoo products |
| **Conditioner Bar** | ~240 products | Solid conditioner products |  
| **Face Serum** | ~265 products | Facial skincare serums |
| **Body Butter** | ~265 products | Body moisturizing products |

**Total**: ~1,030 products from 370+ retailers across 680+ brands

---

## ğŸ” Data Quality Checks

The validation automatically checks:
- âœ… **Primary Keys**: No duplicates in ID columns
- âœ… **Foreign Keys**: All relationships valid  
- âœ… **Data Quality**: No missing required fields
- âœ… **Business Logic**: Expected categories and price ranges
- âœ… **File Completeness**: All required CSV files present

---

## ğŸ¤ Enhanced Product Matching Engine

### Key Performance Metrics
- **56% Coverage**: 837 total matching pairs across 888 products
- **35% Improvement**: Over original lexical-only matching approach
- **Semantic Understanding**: Vector embeddings with Sentence-BERT
- **Post-Processing Recovery**: Additional matches from unmatched products

### Matching Features
1. **Hybrid Scoring**: Combines lexical (60%) + semantic (40%) similarity
2. **Vector Embeddings**: `all-MiniLM-L6-v2` Sentence-BERT model
3. **Embedding Caching**: Persistent cache for improved performance
4. **Multi-Retailer Support**: Matches identical products across different retailers
5. **Quality Tiers**: Perfect matches (â‰¥88%) and similar matches (â‰¥65%)

### Business Impact
- **Better Price Comparison**: 35% more product pairs available
- **Reduced Unmatched Products**: 21.6% fewer products without matches  
- **Enhanced User Experience**: Semantic matching captures product relationships
- **BI-Ready Outputs**: Single consolidated CSV per matching run

### Output Files (Per Run)
- `processed_matches_YYYYMMDD_HHMMSS.csv` - All matching pairs with confidence scores
- `unmatched_products_YYYYMMDD_HHMMSS.csv` - Products still needing matches
- `processing_summary_YYYYMMDD_HHMMSS.json` - Performance metrics and metadata

---

## ğŸ› ï¸ Technical Details

### Environment
- **Python 3.14+** with virtual environment
- **pandas, numpy** for data processing  
- **CSV format** for maximum compatibility

### File Naming
- Raw: `all_search_results_YYYYMMDD_HHMMSS.csv`
- Processed: `{table_name}_YYYYMMDD_HHMMSS.csv`  
- Final: `{table_name}.csv` (ready for import)

### Categories Tracked
1. **Shampoo Bar** - Solid hair cleansing products
2. **Conditioner Bar** - Solid hair conditioning products  
3. **Face Serum** - Concentrated facial treatments
4. **Body Butter** - Rich body moisturizers

---

## ğŸ“ Support

### If Something Goes Wrong:
1. Check the processing report generated in project root
2. Run validation: `python scripts/validate_data.py`
3. Check virtual environment: `source venv/bin/activate`

### Manual Override:
```bash
# If automation fails, run steps manually:
source venv/bin/activate
python src/cleandata_script.py data/raw/[latest_file].csv data/processed  
python scripts/update_database_files.py
python scripts/validate_data.py
```

---

## ğŸ‰ Success Indicators

### Data Processing Pipeline
When everything works correctly, you'll see:
- âœ… **Processing report** generated
- âœ… **All validation checks** passed  
- âœ… **6 CSV files** updated in `database_to_import/`
- âœ… **~1,000+ records** ready for database import

### Product Matching Pipeline
When matching runs successfully, you'll get:
- âœ… **56% product coverage** with 837+ matching pairs
- âœ… **3 timestamped files** in `data/processed/`
- âœ… **Performance summary** in JSON format
- âœ… **BI-ready output** for analytics and reporting

**The complete pipeline delivers both clean data and intelligent product matching! ğŸš€**

---
*Last Updated: November 18, 2025 | Pipeline Version: 2.0 (Enhanced Matching)*
