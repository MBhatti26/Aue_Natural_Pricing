# AuÃª Natural - Product Price Monitoring & Matching Pipeline

**Advanced Product Matching Engine with Semantic Similarity for Competitive Price Intelligence**

---

## ğŸ“‹ Project Overview

This project implements an intelligent product matching and price monitoring system for AuÃª Natural's beauty and personal care products. The pipeline scrapes Google Shopping data, matches products across retailers using advanced semantic similarity algorithms, and generates actionable pricing intelligence for competitive analysis.

### Key Features

- ğŸ” **Google Shopping Scraper** - Automated data collection via Oxylabs API
- ğŸ§  **Hybrid Matching Engine** - Combines lexical (RapidFuzz + Jaccard) and semantic similarity (Sentence-BERT)
- ğŸ·ï¸ **Retailer Standardization** - Normalizes merchant names across data sources
- ğŸ“¦ **Multipack/Variant Parsing** - Intelligent size normalization (e.g., "3 x 50ml" â†’ 150ml)
- ğŸ”„ **Deduplication** - Advanced deduplication with temporal price snapshots
- ğŸ“Š **Power BI Integration** - Automated dashboard data preparation
- ğŸ—„ï¸ **PostgreSQL Database** - Structured data warehouse with schema management

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Google Shopping API                        â”‚
â”‚                      (Oxylabs)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Data Collection & Storage                      â”‚
â”‚            (src/oxylabs_googleshopping_script.py)           â”‚
â”‚                  â†’ data/raw/*.csv                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Data Cleaning & Normalization                    â”‚
â”‚              (src/cleandata_script.py)                       â”‚
â”‚   â€¢ Retailer standardization                                 â”‚
â”‚   â€¢ Multipack parsing                                        â”‚
â”‚   â€¢ Size normalization                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               PostgreSQL Data Warehouse                      â”‚
â”‚                  (database_to_import/)                       â”‚
â”‚   Tables: product, brand, category, retailer,               â”‚
â”‚           stg_price_snapshot                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Enhanced Matching Engine (Hybrid AI)                â”‚
â”‚         (src/enhanced_matching_engine.py)                    â”‚
â”‚   â€¢ Lexical: RapidFuzz + Jaccard similarity                 â”‚
â”‚   â€¢ Semantic: Sentence-BERT embeddings                      â”‚
â”‚   â€¢ Brand-weighted scoring                                   â”‚
â”‚   â€¢ Size-aware matching                                      â”‚
â”‚   â€¢ Category filtering                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Outputs & Analytics                         â”‚
â”‚                                                              â”‚
â”‚  â€¢ data/processed/processed_matches.csv                     â”‚
â”‚  â€¢ data/processed/unmatched_products.csv                    â”‚
â”‚  â€¢ powerbi_data/FINAL_MATCHES.csv                           â”‚
â”‚  â€¢ powerbi_data/FINAL_UNMATCHED.csv                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- PostgreSQL 12+
- Oxylabs API credentials
- Virtual environment (recommended)

### Installation

```bash
# 1. Clone the repository
git clone <repository-url>
cd Final_Project_Aue_Natural

# 2. Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Set up environment variables
cp env.example .env
# Edit .env with your credentials:
#   - OXYLABS_USERNAME
#   - OXYLABS_PASSWORD
#   - PostgreSQL connection details

# 5. Initialize database
psql -U postgres -f sql/create_schema.sql
```

### Running the Pipeline

**Option 1: Complete Pipeline (Recommended)**
```bash
python run_complete_pipeline.py
```

**Option 2: Step-by-Step**
```bash
# Step 1: Scrape Google Shopping data
python src/oxylabs_googleshopping_script.py

# Step 2: Clean and import to database
python src/cleandata_script.py

# Step 3: Run matching engine
python src/enhanced_matching_engine.py

# Step 4: Prepare Power BI data
python scripts/prepare_powerbi_data.py
```

**Option 3: Shell Script**
```bash
./run_all.sh
```

---

## ğŸ“‚ Repository Structure

```
Final_Project_Aue_Natural/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ run_complete_pipeline.py          # Main pipeline orchestrator
â”œâ”€â”€ run_all.sh                        # Shell convenience script
â”œâ”€â”€ .gitignore                        # Git ignore rules
â”œâ”€â”€ .env                              # Environment variables (not in git)
â”œâ”€â”€ env.example                       # Template for .env
â”‚
â”œâ”€â”€ src/                              # Core processing engines
â”‚   â”œâ”€â”€ oxylabs_googleshopping_script.py  # Data scraper
â”‚   â”œâ”€â”€ enhanced_matching_engine.py   # Hybrid AI matcher
â”‚   â”œâ”€â”€ matching_engine.py            # Legacy matcher
â”‚   â”œâ”€â”€ deduplication_manager.py      # Dedup logic
â”‚   â”œâ”€â”€ cleandata_script.py           # Data cleaning
â”‚   â”œâ”€â”€ file_based_enhanced_matcher.py
â”‚   â””â”€â”€ archive/                      # Old versions
â”‚
â”œâ”€â”€ scripts/                          # Utilities & automation
â”‚   â”œâ”€â”€ automated_matching_pipeline.py
â”‚   â”œâ”€â”€ prepare_powerbi_data.py       # Power BI data prep
â”‚   â”œâ”€â”€ run_pipeline.py
â”‚   â”œâ”€â”€ post_process_unmatched.py
â”‚   â”œâ”€â”€ powerbi_auto_update.py
â”‚   â”œâ”€â”€ cleanup_processed.py
â”‚   â”œâ”€â”€ consolidate_outputs.py
â”‚   â”œâ”€â”€ process_new_data.py
â”‚   â”œâ”€â”€ update_database_files.py
â”‚   â”œâ”€â”€ brand_analysis.py
â”‚   â”œâ”€â”€ debug_matching.py
â”‚   â”œâ”€â”€ setup_powerbi.py
â”‚   â”œâ”€â”€ auto_generate_powerbi_dashboards.py
â”‚   â”œâ”€â”€ create_powerbi_quickstart.py
â”‚   â””â”€â”€ archive/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # Raw Google Shopping CSVs
â”‚   â”‚   â””â”€â”€ archive/
â”‚   â”œâ”€â”€ processed/                    # Processed outputs
â”‚   â”‚   â”œâ”€â”€ processed_matches.csv
â”‚   â”‚   â”œâ”€â”€ unmatched_products.csv
â”‚   â”‚   â””â”€â”€ embeddings_cache.pkl      # ML model cache
â”‚   â””â”€â”€ logs/
â”‚
â”œâ”€â”€ powerbi_data/                     # Power BI data exports
â”‚   â”œâ”€â”€ FINAL_MATCHES.csv
â”‚   â””â”€â”€ FINAL_UNMATCHED.csv
â”‚
â”œâ”€â”€ database_to_import/               # Database staging area
â”‚   â”œâ”€â”€ product.csv
â”‚   â”œâ”€â”€ brand_*.csv
â”‚   â”œâ”€â”€ category_*.csv
â”‚   â””â”€â”€ archive/
â”‚
â”œâ”€â”€ sql/                              # Database schema & queries
â”‚   â”œâ”€â”€ create_schema.sql
â”‚   â”œâ”€â”€ mysql_schema.sql
â”‚   â”œâ”€â”€ mysql_import_data.sql
â”‚   â””â”€â”€ archive/
â”‚
â”œâ”€â”€ docs/                             # Documentation
â”‚   â”œâ”€â”€ Cleanup_Summary.md
â”‚   â”œâ”€â”€ Enhanced_Matching_Results.md
â”‚   â”œâ”€â”€ Complete_Pipeline_Run_20251119.md
â”‚   â”œâ”€â”€ Write Up Aue Natural_21154568.docx
â”‚   â”œâ”€â”€ Papers cited/
â”‚   â””â”€â”€ archive/
â”‚
â””â”€â”€ logs/                             # Application logs
    â””â”€â”€ run_pipeline_*.log
```

---

## ğŸ§ª Matching Algorithm

The enhanced matching engine uses a **hybrid scoring approach**:

### Components

1. **Lexical Similarity (60% weight)**
   - RapidFuzz token_set_ratio (fuzzy string matching)
   - Jaccard token overlap (set-based similarity)

2. **Semantic Similarity (40% weight)**
   - Sentence-BERT embeddings (all-MiniLM-L6-v2 model)
   - Cosine similarity on vector representations
   - Cached embeddings for performance

3. **Additional Scoring Factors**
   - Brand matching: +20 (same) / -25 (different)
   - Size similarity: +20 (exact) / +10 (within 20%) / -10 (different units)
   - Subcategory keywords: +10 (match) / -15 (mismatch)
   - Category filtering: Only compare products within same category

### Thresholds

- **Minimum similarity**: 65/100
- **Size tolerance**: Â±20%
- **Perfect match**: 88+/100

### Data Quality Enhancements

âœ… **Retailer Standardization**: "boots uk", "boots.com" â†’ "boots"  
âœ… **Multipack Parsing**: "3 x 50ml" â†’ 150ml  
âœ… **Incomplete Record Filtering**: Drops products missing price, size, or title  
âœ… **Deduplication**: Removes exact duplicates while preserving price history  

---

## ğŸ“Š Outputs

### 1. Matched Products
**Location**: `data/processed/processed_matches.csv`, `powerbi_data/FINAL_MATCHES.csv`

Columns:
- Product IDs, names, brands, categories
- Prices, sizes, retailers for both products
- Similarity scores (overall, lexical, semantic, brand, size)

### 2. Unmatched Products
**Location**: `data/processed/unmatched_products.csv`, `powerbi_data/FINAL_UNMATCHED.csv`

Products with no matches above threshold - kept for catalog completeness but don't affect price optimization.

### 3. Embeddings Cache
**Location**: `data/processed/embeddings_cache.pkl`

Cached sentence embeddings to speed up subsequent runs (automatically managed).

---

## âš™ï¸ Configuration

### Environment Variables (.env)

```bash
# Oxylabs API Credentials
OXYLABS_USERNAME=your_username
OXYLABS_PASSWORD=your_password

# PostgreSQL Database
DB_USER=postgres
DB_PASSWORD=your_password
DB_HOST=localhost
DB_PORT=5432
DB_NAME=aue_warehouse
```

### Matching Thresholds (src/enhanced_matching_engine.py)

```python
MIN_SIMILARITY = 65          # Minimum score for match
PERFECT_THRESHOLD = 88       # Perfect match threshold
SIZE_TOLERANCE = 0.20        # 20% size difference allowed
LEXICAL_WEIGHT = 0.6         # Weight for lexical similarity
SEMANTIC_WEIGHT = 0.4        # Weight for semantic similarity
```

---

## ğŸ”§ Troubleshooting

### Issue: Empty embeddings cache or slow first run
**Solution**: First run will download Sentence-BERT model (~80MB). Subsequent runs use cached embeddings.

### Issue: PostgreSQL connection errors
**Solution**: Verify credentials in `.env` and ensure PostgreSQL is running:
```bash
# Check if PostgreSQL is running
pg_isready

# Restart PostgreSQL (macOS)
brew services restart postgresql@14
```

### Issue: Oxylabs API errors
**Solution**: 
- Check credits in your Oxylabs dashboard
- Verify credentials in `.env`
- Reduce pagination in `src/oxylabs_googleshopping_script.py` if hitting rate limits

### Issue: No matches found
**Solution**:
- Lower `MIN_SIMILARITY` threshold (currently 65)
- Check if categories are properly assigned
- Review `data/processed/unmatched_products.csv` for insights

---

## ğŸ“ˆ Results Summary

**Latest Run (November 28, 2025)**:
- **Input rows**: 960 (raw Google Shopping + catalog data)
- **Unique product-retailer rows**: 159 (after cleaning & deduplication)
- **Matching pairs found**: 77
- **Unmatched SKUs**: 82
- **Data quality**: Significantly improved with enhancements

**Key Improvements**:
- Multipack/variant parsing added
- Retailer name standardization implemented
- Incomplete records filtered out
- Semantic similarity layer added
- Brand and size scoring enhanced

---

## ğŸ“š Documentation

Detailed documentation available in `/docs/`:
- `Enhanced_Matching_Results.md` - Algorithm details and results
- `Complete_Pipeline_Run_20251119.md` - Full pipeline execution log
- `Cleanup_Summary.md` - Data cleaning process
- `Write Up Aue Natural_21154568.docx` - Academic write-up

---

## ğŸ¤ Contributing

1. Create feature branch: `git checkout -b feature/your-feature`
2. Make changes and test
3. Commit: `git commit -m "Add feature"`
4. Push: `git push origin feature/your-feature`
5. Create Pull Request

---

## ğŸ“„ License

This project is part of an academic research project for AuÃª Natural.

---

## ğŸ™ Acknowledgments

- **Oxylabs** - Google Shopping API provider
- **Sentence-Transformers** - Semantic similarity models
- **RapidFuzz** - Fast fuzzy string matching
- **PostgreSQL** - Data warehouse

---

**Last Updated**: December 1, 2025  
**Version**: 2.0 (Enhanced with Semantic Matching)
